{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7a7a271",
   "metadata": {},
   "source": [
    "### Facsimile inversion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b19a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools as it\n",
    "import sys\n",
    "sys.path.append('../Comm/')\n",
    "from utils import iterative_solve_eta as itsol\n",
    "from utils import iterative_solve_Q as itsolQ\n",
    "from utils import create_random_Q0\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0539766f",
   "metadata": {},
   "source": [
    "## Solve eta $\\eta$\n",
    "\n",
    "The value of $\\eta$ is obtained by solving the following equation:\n",
    "$$\n",
    "\\eta = \\left( 1 - \\gamma T \\right)^{-1} \\rho\n",
    "$$\n",
    "\n",
    "Where $\\rho$ is given by the process and $T$ is building as follows:\n",
    "$$\n",
    "T(s',m'|s,m) = \\sum_{a,y}\\, p(s'|s,a)\\, \\pi(a,m'|m,y)\\, f(y|s)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125594d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip = lambda x, l, u: l if x < l else u if x > u else x\n",
    "\n",
    "def solve_eta(pi, PObs_lim, gamma, rho0, Lx, Ly, Lx0, Ly0, find_range):\n",
    "    \"\"\"\n",
    "    This function should solve the following:\n",
    "    --> New_eta = (1 - gamma T)^-1 rho\n",
    "    \"\"\"\n",
    "    \n",
    "    O, M, A = pi.shape\n",
    "    L = Lx * Ly\n",
    "    new_eta = np.zeros(M*L)\n",
    "    \n",
    "    # PY has size ~ 10^5\n",
    "    PY = PObs_lim.reshape(O, M, Ly, Lx)\n",
    "    # PY has size ~ 10^2\n",
    "    PAMU = pi.reshape(O, M, M, A//M)\n",
    "    \n",
    "    p_a_mu_m_xy = np.einsum( 'omyx, omna -> anmyx', PY, PAMU)\n",
    "    # T [ s'm'  sm] = sum_a, mu p(s'm' | sm a mu) p(a mu | sm)\n",
    "    #               = sum_a, mu p(s'm' | sm a mu) sum_y f(y | s) pi(a mu | y m)\n",
    "    \n",
    "    print(p_a_mu_m_xy.shape)\n",
    "    \n",
    "    # Tsm_sm has size ~ 10^5 x 10^5 or more\n",
    "    Tsm_sm = np.zeros( (M, Ly, Lx, M, Ly, Lx) )\n",
    "    \n",
    "    # Action Order\n",
    "    # left, right, up, down\n",
    "    \n",
    "    # tuples to be populated\n",
    "    #      T indices : \"left / right / up / down\"\n",
    "    #      (m', y', x', m, y, x)\n",
    "    #      pi indices : \"left_act / ... \"\n",
    "    #      (m', a, m, y, x)\n",
    "    for im_new in range(M):\n",
    "        \n",
    "        left = [ (im_new, iy, clip(ix-1, 0, Lx-1), im, iy, ix)   for ix in np.arange(Lx) for iy in np.arange(Ly) for im in np.arange(M)]\n",
    "        left_act = [ (0, im_new, im, iy, ix)   for ix in np.arange(Lx) for iy in np.arange(Ly) for im in np.arange(M)]\n",
    "        \n",
    "        for l, la in zip(left, left_act):\n",
    "            Tsm_sm[l] += p_a_mu_m_xy[la]\n",
    "        \n",
    "        right = [ (im_new, iy, clip(ix+1, 0, Lx-1), im, iy, ix)   for ix in np.arange(Lx) for iy in np.arange(Ly) for im in np.arange(M)]\n",
    "        right_act = [ (1, im_new, im, iy, ix)   for ix in np.arange(Lx) for iy in np.arange(Ly) for im in np.arange(M)]\n",
    "        for r, ra in zip(right, right_act):\n",
    "            Tsm_sm[r] += p_a_mu_m_xy[ra]\n",
    "    \n",
    "        up = [ (im_new, clip(iy+1, 0, Ly-1), ix,  im, iy, ix)   for ix in np.arange(Lx) for iy in np.arange(Ly) for im in np.arange(M)]\n",
    "        up_act = [ (2, im_new, im, iy, ix)   for ix in np.arange(Lx) for iy in np.arange(Ly) for im in np.arange(M)]\n",
    "        for u, ua in zip(up, up_act):\n",
    "            Tsm_sm[u] += p_a_mu_m_xy[ua]\n",
    "        \n",
    "        down = [ (im_new, clip(iy-1, 0, Ly-1), ix, im, iy, ix)   for ix in np.arange(Lx) for iy in np.arange(Ly) for im in np.arange(M)]\n",
    "        down_act = [ (3, im_new, im, iy, ix)   for ix in np.arange(Lx) for iy in np.arange(Ly) for im in np.arange(M)]\n",
    "        for d, da in zip(down, down_act):\n",
    "            Tsm_sm[d] += p_a_mu_m_xy[da]\n",
    "    \n",
    "    yxs = it.product(np.arange(Ly), np.arange(Lx))\n",
    "    yx_founds = it.filterfalse(lambda x: (x[0]-Ly0)**2 + (x[1]-Lx0)**2 > find_range**2, yxs)\n",
    "    \n",
    "    All = slice(None)\n",
    "    for yx_found in yx_founds:\n",
    "        ls = (All, yx_found[0], yx_found[1], All, All, All)\n",
    "        # all transitions starting from the source do not go anywhere\n",
    "        Tsm_sm[ls] = 0\n",
    "        # all transitions ending in the source stop the episode\n",
    "        ls = (All, All, All, All, yx_found[0], yx_found[1])\n",
    "        Tsm_sm[ls] = 0\n",
    "    \n",
    "    Tsm_sm_matrix = np.reshape(Tsm_sm, (M*Ly*Lx, M*Ly*Lx))\n",
    "    to_invert = np.eye(M*Ly*Lx) - gamma * Tsm_sm_matrix\n",
    "    inverted = np.linalg.inv(to_invert)\n",
    "    new_eta = inverted @ rho0\n",
    "    \n",
    "    return new_eta, Tsm_sm_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d2b25db0",
   "metadata": {},
   "source": [
    "## Solve Q \n",
    "The value of $Q$ is the following:\n",
    "$$\n",
    "Q(s,m,a,m')= \\sum_{s'}\\, p(s'|s,a)\\, \\mathbb{1}(s'\\neq x_s)\\,(\\gamma V(s',m')-1\\,)\n",
    "$$\n",
    "\n",
    "First we need to find $V$ solving the linear system:\n",
    "$$\n",
    "\\begin{align*}\n",
    "V^{T} &= r^{T}(1-\\gamma T)^{-1} \\\\\n",
    "V^{T}(1-\\gamma T)  &= r^{T} \\\\\n",
    "(1-\\gamma T^{T})V  &= r \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "Where $r$ is the reward vector, $T$ is the transition matrix and $\\gamma$ is the discount factor.\n",
    "\n",
    "The reward vector is:\n",
    "$$\n",
    "r(s,m) = -(1-\\gamma) \\sum_{a,s',y}\\, p(s'|s,a)\\, f(y|s) \\, \\pi_\\theta (a,m'|m,y)\\, \\mathbb{1}(x'\\neq x_s) \n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d0f8c89",
   "metadata": {},
   "source": [
    "## Questions:\n",
    "Why in the paper the T matrix looks like same for solve Q and eta?\n",
    "\n",
    "Why the average rewards from Fortran have a lot of dimensions (M,Lx,Ly,A,M)?\n",
    "\n",
    "Is the same average reward that you get from Fortran and the paper?\n",
    "\n",
    "First we need to compute V, make the inversion, and after build Q?\n",
    "\n",
    "What is the difference between m' and mu?\n",
    "\n",
    "What is PY  and PAMU in the code?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff2dc6f4",
   "metadata": {},
   "source": [
    "## New set of questions:\n",
    "\n",
    "Why when build T matrix exclude the source point? In the paper is not excluded.\n",
    "\n",
    "To build the reward vector, Could I take the T matrix and sum over s' and m' for each s and m after discard the source and multiply by 1-gamma?\n",
    "$$\n",
    "r(s,m) = -(1-\\gamma) \\sum_{s',m'}\\, T(s',m'|s,m) \\mathbb{1}(x'\\neq x_s)\n",
    "$$\n",
    "\n",
    "When is solved the linear system, the V(s,m) becomes to V(s',m')?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed9a85cf",
   "metadata": {},
   "source": [
    "## New New set of questions:\n",
    "\n",
    "Is necessary to include the reward find in the building of average reward vector?\n",
    "\n",
    "Is the same restriction $1(s' \\neq x_s)$ and $1(x' \\neq x_s)$\n",
    "\n",
    "In Q vector, m and m' are the same?\n",
    "\n",
    "How make the sum over s' in Q vector?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7aaadddd",
   "metadata": {},
   "source": [
    "## New New New set of questions:\n",
    "\n",
    "Why if I omit the -1 in the Q vector, the results are better?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1c8f845f",
   "metadata": {},
   "source": [
    "## $\\text{New}^4$ set of questions:\n",
    "Why is necessary to put pi to zero to have the same values?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "302b7ff4",
   "metadata": {},
   "source": [
    "## Set of actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b3efbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentActions():\n",
    "    def __init__(self,actions_number):\n",
    "        space_dim = 2\n",
    "        self.A = actions_number\n",
    "        self.actions = np.zeros((actions_number,2),dtype=int)\n",
    "        self.actions_names = {}\n",
    "    \n",
    "    def set_action(self,action_index : int, action : np.ndarray, action_name : str ):\n",
    "        self.actions[action_index] = action\n",
    "        self.actions_names[action_index] = action_name\n",
    "\n",
    "    def action_move(self,action_index : int):\n",
    "        if action_index >= self.A:\n",
    "            print('Error: action not recognized')\n",
    "            return np.array([0, 0])\n",
    "        return self.actions[action_index]\n",
    "\n",
    "def get_next_state(state : np.ndarray, action : int, act_handle : AgentActions, move = None):\n",
    "\n",
    "    if move is None :\n",
    "        move = act_handle.action_move(action)\n",
    "\n",
    "    n_rolls = [0,0]\n",
    "\n",
    "    clipped_x = []\n",
    "    dataClipped_x = None\n",
    "    clipped_y = []\n",
    "    dataClipped_y = None\n",
    "\n",
    "    # Complex move\n",
    "    if move[0] != 0 and move[1] != 0 :\n",
    "        state = get_next_state(state,-1,move=[0,move[-1]])\n",
    "        state = get_next_state(state,-1,move=[move[-2],0])\n",
    "\n",
    "    # move in x\n",
    "    elif move[-1] != 0 : \n",
    "        col = move[-1]\n",
    "        if col < 0 :\n",
    "            data2clip = np.s_[:,0]\n",
    "            clipped_x = [i for i in range(-col)]\n",
    "        else :\n",
    "            data2clip = np.s_[:,-1]\n",
    "            clipped_x = [-i for i in range(1,col+1)]\n",
    "\n",
    "        dataClipped_x = state[data2clip].copy()\n",
    "\n",
    "        n_rolls[-1] = -col\n",
    "\n",
    "    # move in y \n",
    "    elif move[-2] != 0 : \n",
    "        row = move[-2]\n",
    "        if row < 0 :\n",
    "            data2clip = np.s_[0,:]\n",
    "            clipped_y = [i for i in range(-row)]\n",
    "        else :\n",
    "            data2clip = np.s_[-1,:]\n",
    "            clipped_y = [-i for i in range(1,row+1)]\n",
    "\n",
    "        dataClipped_y = state[data2clip].copy()\n",
    "\n",
    "        n_rolls[-2] = -row\n",
    "\n",
    "    ## move in z\n",
    "\n",
    "    state = np.roll(state,n_rolls,axis=(0,1))\n",
    "\n",
    "    for i in clipped_y :\n",
    "        state[i,:] = dataClipped_y\n",
    "    for i in clipped_x :\n",
    "        state[:,i] = dataClipped_x\n",
    "\n",
    "    return state\n",
    "\n",
    "act_hdl = AgentActions(4)\n",
    "act_hdl.set_action(0, np.array([0,-1]), 'left')\n",
    "act_hdl.set_action(1, np.array([0,1]), 'right')\n",
    "act_hdl.set_action(2, np.array([1,0]), 'up')\n",
    "act_hdl.set_action(3, np.array([-1,0]), 'down')\n",
    "\n",
    "tmp = np.arange(5*7).reshape(7,5)\n",
    "mask_source = [(4,2),(3,2),(5,2),(4,1),(4,3)]\n",
    "for i in mask_source :\n",
    "    tmp[i] = 0\n",
    "tmp[4,2] = -1\n",
    "print(\"Initial \\n\",tmp)\n",
    "output = get_next_state(tmp,0,act_hdl)\n",
    "print(\"Final\\n\",output)\n",
    "\n",
    "print(\"Mask the source\")\n",
    "for i in mask_source :\n",
    "    output[i] = 0\n",
    "print(\"Masked\\n\",output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1831000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_reward(Tsm_sm_matrix, M, Lx, Ly, cost_move,source_as_zero):\n",
    "    \"\"\"\n",
    "    This function compute the average reward from the Transition matrix T\n",
    "    The dimension of RR is (M, Ly, Lx)\n",
    "    \"\"\"\n",
    "    # Tsm_sm = Tsm_sm_matrix.reshape(M, Ly, Lx, M, Ly, Lx)\n",
    "    # RR = -cost_move * Tsm_sm.sum(axis=(0,1,2))\n",
    "\n",
    "    RR = np.full((M, Ly, Lx),-cost_move)\n",
    "    RR[:,source_as_zero[:,0],source_as_zero[:,1]] = 0.0\n",
    "\n",
    "    return RR\n",
    "\n",
    "def solve_Q(Tsm_sm_matrix, Lx, Ly, Lx0, Ly0, M, A, gamma,find_range, act_hdl):\n",
    "\n",
    "    yxs = it.product(np.arange(Ly), np.arange(Lx))\n",
    "    yx_founds = it.filterfalse(lambda x: (x[0]-Ly0)**2 + (x[1]-Lx0)**2 > find_range**2, yxs)\n",
    "    source_as_zero = np.array([i for i in yx_founds])\n",
    "\n",
    "    cost_move = 1.0 - gamma\n",
    "    reward = average_reward(Tsm_sm_matrix, M, Lx, Ly, cost_move,source_as_zero)\n",
    "\n",
    "    reward = reward.reshape(M*Ly*Lx)\n",
    "\n",
    "    Tsm_sm_matrix = Tsm_sm_matrix.transpose()\n",
    "\n",
    "    V = np.linalg.solve(np.eye(M*Ly*Lx) - gamma * Tsm_sm_matrix, reward)\n",
    "\n",
    "    V = gamma * V \n",
    "    # V = gamma * V - 1.0\n",
    "\n",
    "    V = V.reshape(M, Ly, Lx)\n",
    "\n",
    "    state = np.empty((Ly, Lx))\n",
    "\n",
    "    Q = np.zeros((Ly, Lx, M, A))\n",
    "    for im in range(M):\n",
    "        for a in range(A):\n",
    "            state[:,:] = V[im,:,:]\n",
    "            new_state = get_next_state(state, a, act_hdl)\n",
    "            new_state = new_state - cost_move\n",
    "            new_state[source_as_zero[:,0],source_as_zero[:,1]] = 0\n",
    "            Q[:,:,im, a] = new_state\n",
    "\n",
    "    Q = np.repeat(Q[np.newaxis,:,:,:,:], M,axis=0)\n",
    "            \n",
    "    return V, Q"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e6f4239",
   "metadata": {},
   "source": [
    "## Compute the reward vector $r$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada5ac07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_move_0(x,y,a):\n",
    "    if a == 0:\n",
    "        return x+1,y\n",
    "    if a == 1:\n",
    "        return x-1,y\n",
    "    if a == 2:\n",
    "        return x,y-1\n",
    "    if a == 3:\n",
    "        return x,y+1\n",
    "    if a == 4:\n",
    "        return x,y\n",
    "    \n",
    "def average_reward_fortran(Lx,Ly,M,A,Lx0,Ly0,find_range,cost_move,reward_find):\n",
    "    \"\"\"\n",
    "    Lx,Ly: size of the grid\n",
    "    M: number of memory states\n",
    "    A: number of actions\n",
    "    Lx0,Ly0: location of the agent\n",
    "    find_range: range of the agent's sensor\n",
    "    cost_move: cost of moving\n",
    "    reward_find: reward for finding the source\n",
    "\n",
    "    Returns:\n",
    "    RR_np: average reward matrix\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    RR_np = np.ones((Ly,Lx,A)) * (- cost_move)\n",
    "    # Set zero inside of the source area\n",
    "    radius = find_range**2\n",
    "    y_range = (int(Ly0-radius)-2,int(Ly0+radius)+2)\n",
    "    x_range = (int(Lx0-radius)-2,int(Lx0+radius)+2)\n",
    "\n",
    "    # print(\"radius\",radius,\"y_range\",y_range,\"x_range\",x_range)\n",
    "\n",
    "    for i in range(y_range[0],y_range[1]):\n",
    "        for j in range(x_range[0],x_range[1]):\n",
    "            if (i-Ly0)**2 + (j-Lx0)**2 < radius:\n",
    "                for a in range(A):\n",
    "                    x,y = action_move_0(j,i,a)\n",
    "                    RR_np[y,x,a] += reward_find\n",
    "\n",
    "    for i in range(y_range[0],y_range[1]):\n",
    "        for j in range(x_range[0],x_range[1]):\n",
    "            if (i-Ly0)**2 + (j-Lx0)**2 < radius:\n",
    "                RR_np[i,j,:] = 0.0\n",
    "\n",
    "    RR_np = np.tile(RR_np, M)\n",
    "    RR_np = RR_np.reshape(Ly,Lx,M,A)\n",
    "    RR_np = np.repeat(RR_np[np.newaxis,:,:,:,:], M,axis=0)\n",
    "    # RR_np = RR_np.flatten()\n",
    "\n",
    "    return RR_np\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e81b5fe3",
   "metadata": {},
   "source": [
    "## TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28d0b10",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "M = 2\n",
    "O = 2\n",
    "Lx = 11*2\n",
    "Lx = 27\n",
    "Ly = 15*2\n",
    "Ly = 39\n",
    "a_size = 4\n",
    "\n",
    "Lx0 = Lx // 2\n",
    "Ly0 = Ly // 3\n",
    "\n",
    "Lx0 = 5\n",
    "Lx0 = 13\n",
    "Ly0 = 7\n",
    "Ly0 = 27\n",
    "\n",
    "gamma = 0.9\n",
    "# gamma = 0.99\n",
    "# gamma = 0.99975   \n",
    "gamma = 0.99999\n",
    "\n",
    "find_range = 1.1\n",
    "tol = 1e-8\n",
    "\n",
    "# Average reward\n",
    "cost_move = 1-gamma\n",
    "# reward_find = cost_move * 0.8\n",
    "reward_find = 0.0\n",
    "\n",
    "# Create an action handler\n",
    "act_hdl = AgentActions(a_size)\n",
    "act_hdl.set_action(0, np.array([0,-1]), 'left')\n",
    "act_hdl.set_action(1, np.array([0,1]), 'right')\n",
    "act_hdl.set_action(2, np.array([1,0]), 'up')\n",
    "act_hdl.set_action(3, np.array([-1,0]), 'down')\n",
    "\n",
    "np.random.seed(33)\n",
    "\n",
    "pi = softmax( np.random.rand(O,M,M*a_size), 2)\n",
    "\n",
    "PObs_lim = np.random.rand(O, M*Lx*Ly)\n",
    "PObs_lim[1] = 1-PObs_lim[0]\n",
    "rho0 = np.random.rand(M*Lx*Ly)\n",
    "rho0[Lx:] = 0\n",
    "rho0 /= np.sum(rho0)\n",
    "eta0 = np.random.rand(M*Lx*Ly)\n",
    "\n",
    "# Solve eta\n",
    "inv_sol, T = solve_eta(pi, PObs_lim, gamma, rho0, Lx, Ly, Lx0, Ly0, find_range)\n",
    "iter_sol = itsol(pi, PObs_lim, gamma, rho0, eta0, tol, Lx, Ly, Lx0, Ly0, find_range)\n",
    "plt.imshow(T)\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8701be35",
   "metadata": {},
   "source": [
    "# $Q$ vector from linear solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12151a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Solve Q\n",
    "\n",
    "V, Q = solve_Q(T, Lx, Ly, Lx0, Ly0, M, a_size, gamma, find_range,act_hdl)\n",
    "print('V shape', V.shape)\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "for i in range(M):\n",
    "    plt.subplot(1,M,i+1)\n",
    "    plt.title(\"Expected value at M = {}\".format(i))\n",
    "    plt.imshow(V[i,:,:])\n",
    "    plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "print('Q shape', Q.shape)\n",
    "for i in range(M):\n",
    "    fig = plt.figure(figsize=(16,4))\n",
    "    for j in range(a_size):\n",
    "        plt.subplot(1,a_size,j+1)\n",
    "        plt.title(\"M: {}, Action: {}\".format(i,j))\n",
    "        plt.imshow(Q[0,:,:,i,j])\n",
    "        plt.colorbar()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Diff between Q in memory state 0 and 1\n",
    "print(\"Diff between Q in memory state 0 and 1\")\n",
    "_ = [print(\"Action:\",j,np.sum(np.abs(Q[0,:,:,0,j]-Q[0,:,:,1,j]))) for j in range(a_size)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09523b04",
   "metadata": {},
   "source": [
    "## $Q$ vector from iterative process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aea9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q0 = create_random_Q0(Lx, Ly, Lx0, Ly0, gamma,a_size*M, M, cost_move, reward_find)\n",
    "\n",
    "Q0 = Q0.flatten()\n",
    "RR_fortran = average_reward_fortran(Lx,Ly,M,a_size,Lx0,Ly0,find_range,cost_move,reward_find)\n",
    "RR_fortran = RR_fortran.flatten()\n",
    "Q_fortran = itsolQ(pi, PObs_lim, gamma, RR_fortran, Q0, tol, Lx, Ly, Lx0, Ly0, find_range, cost_move)\n",
    "\n",
    "Q_fortran = Q_fortran.reshape((M,Ly,Lx,M,a_size))\n",
    "\n",
    "print('Q fortran shape', Q_fortran.shape)\n",
    "for i in range(M):\n",
    "    fig = plt.figure(figsize=(16,4))\n",
    "    for j in range(a_size):\n",
    "        plt.subplot(1,a_size,j+1)\n",
    "        plt.title(\"M: {}, Action: {}\".format(i,j))\n",
    "        plt.imshow(Q_fortran[0,:,:,i,j])\n",
    "        plt.colorbar()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Diff between Q in memory state 0 and 1\n",
    "print(\"Diff between Q fortran in memory state 0 and 1\")\n",
    "_ = [print(\"Action:\",j,np.sum(np.abs(Q_fortran[0,:,:,0,j]-Q_fortran[0,:,:,1,j]))) for j in range(a_size)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e6607b",
   "metadata": {},
   "source": [
    "## Diff between $Q$ vector form iteration and $Q$ vector from inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0de73ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Q fortran vs inverse\n",
    "print(\"Diff between Q fortran vs inverse\")\n",
    "for i in range(M):\n",
    "    for j in range(a_size):\n",
    "        print(\"Memory state: {}, Action: {}, Diff: {}\".format(i,j,np.sum(np.abs(Q[0,:,:,i,j]-Q_fortran[0,:,:,i,j]))))\n",
    "        print(\"    Inv min: {}, max: {}\".format(np.min(Q[0,:,:,i,j]), np.max(Q[0,:,:,i,j])))\n",
    "        print(\"Fortran min: {}, max: {}\".format(np.min(Q_fortran[0,:,:,i,j]), np.max(Q_fortran[0,:,:,i,j])))\n",
    "        fig = plt.figure(figsize=(12,4))\n",
    "        plt.subplot(1,4,1)\n",
    "        plt.title(\"Inverse M: {}, Action: {}\".format(i,j))\n",
    "        plt.imshow(Q[0,:,:,i,j])\n",
    "        plt.colorbar()\n",
    "        plt.subplot(1,4,2)\n",
    "        plt.title(\"Fortran M: {}, Action: {}\".format(i,j))\n",
    "        plt.imshow(Q_fortran[0,:,:,i,j])\n",
    "        plt.colorbar()\n",
    "        plt.subplot(1,4,3)\n",
    "        plt.title(\"|Q_inv - Q_ite|\".format(i,j))\n",
    "        plt.imshow(np.abs(Q[0,:,:,i,j]-Q_fortran[0,:,:,i,j]))\n",
    "        plt.colorbar()\n",
    "        plt.subplot(1,4,4)\n",
    "        plt.title(\"-1/(1+diff)+1\")\n",
    "        plt.imshow(-Q[0,:,:,i,j]/Q_fortran[0,:,:,i,j]+1.0)\n",
    "        plt.colorbar()\n",
    "\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80e4b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "for im in range(M):\n",
    "    fig = plt.figure(figsize=(12,4))\n",
    "    plt.subplot(1,4,1)\n",
    "    plt.title(\"Inverse\")\n",
    "    plt.imshow(inv_sol.reshape(M,Ly,Lx)[im])\n",
    "    plt.colorbar()\n",
    "    plt.subplot(1,4,2)\n",
    "    plt.title(\"Iterative\")\n",
    "    plt.imshow(iter_sol.reshape(M,Ly,Lx)[im])\n",
    "    plt.colorbar()\n",
    "    plt.subplot(1,4,3)\n",
    "    plt.title(\"Diff | M = {}\".format(im))\n",
    "    plt.imshow(np.abs(iter_sol.reshape(M,Ly,Lx)[im]-inv_sol.reshape(M,Ly,Lx)[im]))\n",
    "    plt.colorbar()\n",
    "    plt.subplot(1,4,4)\n",
    "    plt.title(\"-1/(1+diff)+1\")\n",
    "    plt.imshow(-iter_sol.reshape(M,Ly,Lx)[im]/inv_sol.reshape(M,Ly,Lx)[im]+1.0)\n",
    "    plt.colorbar()\n",
    "\n",
    "    print(np.abs(iter_sol.reshape(M,Ly,Lx)[im]-inv_sol.reshape(M,Ly,Lx)[im]).max())\n",
    "    \n",
    "\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
